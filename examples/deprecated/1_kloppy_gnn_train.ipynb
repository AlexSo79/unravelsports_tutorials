{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåÄ unravel kloppy into graph neural network!\n",
    "\n",
    "First run `pip install unravelsports` if you haven't already!\n",
    "\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unravelsports --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this in-depth walkthrough we'll discuss everything the `unravelsports` package has to offer for converting a [Kloppy](https://github.com/PySport/kloppy) dataset of soccer tracking data into graphs for training binary classification graph neural networks using the [Spektral](https://graphneural.network/) library.\n",
    "\n",
    "This walkthrough will touch on a lot of the concepts from [A Graph Neural Network Deep-dive into Successful Counterattacks {A. Sahasrabudhe & J. Bekkers}](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn). It is strongly advised to first read the [research paper (pdf)](https://ussf-ssac-23-soccer-gnn.s3.us-east-2.amazonaws.com/public/Sahasrabudhe_Bekkers_SSAC23.pdf). Some concepts are also explained in the [Graphs FAQ](graphs_faq.md).\n",
    "\n",
    "Step by step we'll show how this package can be used to load soccer positional (tracking) data with `kloppy`, how to convert this data into \"graphs\", train a Graph Neural Network with `spektral`, evaluate it's performance, save and load the model and finally apply the model to unseen data to make predictions.\n",
    "\n",
    "The powerful Kloppy package allows us to load and standardize data from many providers: Metrica, Sportec, Tracab, SecondSpectrum, StatsPerform and SkillCorner. In this guide we'll use some matches from the [Public SkillCorner Dataset](https://github.com/SkillCorner/opendata).\n",
    "\n",
    "<br>\n",
    "<i>Before we get started it is important to note that the <b>unravelsports</b> library does not have built in functionality to create binary labels, these will need to be supplied by the reader. In this example we use the <b>dummy_labels()</b> functionality that comes with the package. This function creates a single binary label for each frame by randomly assigning it a 0 or 1 value.\n",
    "</i>\n",
    "<br>\n",
    "\n",
    "##### **Contents**\n",
    "\n",
    "- [**1. Imports**](#1-imports).\n",
    "- [**2. Public SkillCorner Data**](#2-public-skillcorner-data).\n",
    "- [**3. Graph Converter**](#2-open-skillcorner-data).\n",
    "- [**4. Load Kloppy Data, Convert & Store**](#4-load-kloppy-data-convert-and-store).\n",
    "- [**5. Creating a Custom Graph Dataset**](#5-creating-a-custom-graph-dataset).\n",
    "- [**6. Prepare for Training**](#6-prepare-for-training).\n",
    "    - [6.1 Split Dataset](#61-split-dataset)\n",
    "    - [6.2 Model Configurations](#62-model-configurations)\n",
    "    - [6.3 Build GNN Model](#63-build-gnn-model)\n",
    "    - [6.4 Create DataLoaders](#64-create-dataloaders)\n",
    "- [**7. GNN Training + Prediction**](#7-training-and-prediction).\n",
    "    - [7.1 Compile Model](#71-compile-model)\n",
    "    - [7.2 Fit Model](#72-fit-model)\n",
    "    - [7.3 Save & Load Model](#73-save--load-model)\n",
    "    - [7.4 Evaluate Model](#74-evaluate-model)\n",
    "    - [7.5 Predict on New Data](#75-predict-on-new-data)\n",
    "\n",
    "‚ÑπÔ∏è [**Graphs FAQ**](graphs_faq.md)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports\n",
    "\n",
    "We import `SoccerGraphConverter` to help us convert from Kloppy positional tracking frames to graphs.\n",
    "\n",
    "With the power of **Kloppy** we can also load data from many providers by importing `metrica`, `sportec`, `tracab`, `secondspectrum`, or `statsperform` from `kloppy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unravel.soccer import SoccerGraphConverter\n",
    "\n",
    "from kloppy import skillcorner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Public SkillCorner Data\n",
    "\n",
    "The `SoccerGraphConverter` class allows processing data from every tracking data provider supported by [PySports Kloppy](https://github.com/PySport/kloppy), namely:\n",
    "- Sportec\n",
    "- Tracab\n",
    "- SecondSpectrum\n",
    "- SkillCorner\n",
    "- StatsPerform\n",
    "- Metrica\n",
    "\n",
    "In this example we're going to use a sample of tracking data from 4 matches of [publicly available SkillCorner data](https://github.com/SkillCorner/opendata). \n",
    "\n",
    "All we need to know for now is that this data is from the following matches:\n",
    "\n",
    "|  id | date_time           | home_team   | away_team   |\n",
    "|---:|:---------------------:|:-----------------------|:-----------------------|\n",
    "|  4039 | 2020-07-02T19:15:00Z | Manchester City        | Liverpool              |\n",
    "|  3749 | 2020-05-26T16:30:00Z | Dortmund               | Bayern Munchen         |\n",
    "|  3518 | 2020-03-08T19:45:00Z | Juventus               | Inter                  |\n",
    "|  3442 | 2020-03-01T20:00:00Z | Real Madrid            | FC Barcelona           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Graph Converter\n",
    "\n",
    "‚ÑπÔ∏è For more information on:\n",
    "- What a Graph is, check out [Graph FAQ Section A](graphs_faq.ipynb)\n",
    "- What parameters we can pass to the `SoccerGraphConverter`, check out [Graph FAQ Section B](graphs_faq.ipynb)\n",
    "- What features each Graph has, check out [Graph FAQ Section C](graphs_faq.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "To get started with the `SoccerGraphConverter` we need to pass one _required_ parameter:\n",
    "- `dataset` (of type `TrackingDataset` (Kloppy)) \n",
    "\n",
    "And one parameter that's required when we're converting for training purposes (more on this later):\n",
    "- `labels` (a dictionary with `frame_id`s as keys and a value of `{True, False, 1 or 0}`).\n",
    "```python\n",
    "{83340: True, 83341: False, etc..} = {83340: 1, 83341: 0, etc..} =  {83340: 1, 83341: False, etc..}\n",
    "```\n",
    "‚ö†Ô∏è As mentioned before you will need to create your own labels! In this example we'll use `dummy_labels(dataset)` to generate a fake label for each frame.\n",
    "\n",
    "#### Graph Identifier(s):\n",
    "When training a model on tracking data it's highly recommended to split data into test/train(/validation) sets by match or period such that all data end up in the same test, train or validation set. This should be done to avoid leaking information between test, train and validation sets. To make this simple, there are two _optional_ parameters we can pass to `SoccerGraphConverter`, namely:\n",
    "- `graph_id`. This is a single identifier (str or int) for a whole match, for example the unique match id.\n",
    "- `graph_ids`. This is a dictionary with the same keys as `labels`, but the values are now the unique identifiers. This option can be used if we want to split by sequence or possession_id. For example: {frame_id: 'matchId-sequenceId', frame_id: 'match_Id-sequenceId2'} etc. You will need to create your own ids. Note, if `labels` and `graph_ids` don't have the exact same keys it will throw an error.\n",
    "\n",
    "In this example we'll use the `graph_id=match_id` as the unique identifier, but feel free to change that for `graph_ids=dummy_graph_ids(dataset)` to test out that behavior.\n",
    "\n",
    "Correctly splitting the final dataset in train, test and validiation sets using these Graph Identifiers is incorporated into `GraphDataset` (see [Section 6.1](#61-split-dataset) for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Load Kloppy Data, Convert and Store\n",
    "\n",
    "As mentioned in [Section 2](#2-public-skillcorner-data) we will use 4 matches of SkillCorner data. In the below example we will load the first 500 frames of data from each of these 4 games (we set `limit=500`) to create a dataset of 2,000 samples (Note: We're going to actually have less than 2,000 samples because setting `include_empty_frames=False` means we'll skip some frames in our conversion step).\n",
    "\n",
    "Important things to note:\n",
    "- We import `dummy_labels` to randomly generate binary labels. Training with these random labels will not create a good model.\n",
    "- We import `dummy_graph_ids` to generate fake graph labels.\n",
    "- The `SoccerGraphConverter` handles all necessary steps (like setting the correct coordinate system, and left-right normalization).\n",
    "- We will end up with fewer than 2,000 eventhough we set `limit=500` frames because we set `include_empty_frames=False` and all frames without ball coordinates are automatically ommited.\n",
    "- When using other providers always set `include_empty_frames=False` or `only_alive=True`.\n",
    "- We store the data as individual compressed pickle files, one file for per match. The data that gets stored in the pickle is a list of dictionaries, one dictionary per frame. Each dictionary has keys for the adjacency matrix, node features, edge features, label and graph id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:02<00:00, 244.81it/s]\n",
      "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 285.65it/s]\n",
      "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 343.58it/s] \n",
      "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 285.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "\n",
    "from unravel.utils import dummy_labels, dummy_graph_ids\n",
    "\n",
    "match_ids = [4039, 3749, 3518, 3442]\n",
    "pickle_folder = \"pickles\"\n",
    "compressed_pickle_file_path = \"{pickle_folder}/{match_id}.pickle.gz\"\n",
    "\n",
    "for match_id in match_ids:\n",
    "    match_pickle_file_path = compressed_pickle_file_path.format(\n",
    "        pickle_folder=pickle_folder, match_id=match_id\n",
    "    )\n",
    "    # if the output file already exists, skip this whole step\n",
    "    if not exists(match_pickle_file_path):\n",
    "\n",
    "        # Load Kloppy dataset\n",
    "        dataset = skillcorner.load_open_data(\n",
    "            match_id=match_id,\n",
    "            coordinates=\"secondspectrum\",\n",
    "            include_empty_frames=False,\n",
    "            limit=500,  # limit to 500 frames in this example\n",
    "        )\n",
    "\n",
    "        # Initialize the Graph Converter, with dataset, labels and settings\n",
    "        converter = SoccerGraphConverter(\n",
    "            dataset=dataset,\n",
    "            # create fake labels\n",
    "            labels=dummy_labels(dataset),\n",
    "            graph_id=match_id,\n",
    "            # graph_ids=dummy_graph_ids(dataset),\n",
    "            # Settings\n",
    "            ball_carrier_treshold=25.0,\n",
    "            max_player_speed=12.0,\n",
    "            max_ball_speed=28.0,\n",
    "            boundary_correction=None,\n",
    "            self_loop_ball=True,\n",
    "            adjacency_matrix_connect_type=\"ball\",\n",
    "            adjacency_matrix_type=\"split_by_team\",\n",
    "            label_type=\"binary\",\n",
    "            infer_ball_ownership=True,\n",
    "            infer_goalkeepers=True,\n",
    "            defending_team_node_value=0.1,\n",
    "            non_potential_receiver_node_value=0.1,\n",
    "            random_seed=False,\n",
    "            pad=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "        # Compute the graphs and directly store them as a pickle file\n",
    "        converter.to_pickle(file_path=match_pickle_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è For a full table of parameters we can pass to the `SoccerGraphConverter` check out [Graph FAQ Section B](graphs_faq.ipynb)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating a Custom Graph Dataset\n",
    "\n",
    "To easily train our model with the Spektral library we need to use a Spektral dataset object. The `GraphDataset` class helps us create such an object really easily.\n",
    "\n",
    "- `GraphDataset` is a [`spektral.data.Dataset`](https://graphneural.network/creating-dataset/). \n",
    "This type of dataset makes it very easy to properly load, train and predict with a Spektral GNN.\n",
    "- The `GraphDataset` has an option to load from a folder of compressed pickle files, all we have to do is pass the pickle_folder location.\n",
    "\n",
    "‚ÑπÔ∏è For more information on the `GraphDataset` please check the [Graphs FAQ Section D](graphs_faq.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unravel.utils import GraphDataset\n",
    "\n",
    "dataset = GraphDataset(pickle_folder=pickle_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare for Training\n",
    "\n",
    "Now that we have all the data converted into Graphs inside our `GraphDataset` object, we can prepare to train the GNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Split Dataset\n",
    "\n",
    "Our `dataset` object has two custom methods to help split the data into train, test and validation sets.\n",
    "Either use `dataset.split_test_train()` if we don't need a validation set, or `dataset.split_test_train_validation()` if we do also require a validation set.\n",
    "\n",
    "We can split our data 'by_graph_id' if we have provided Graph Ids in our `SoccerGraphConverter` using the 'graph_id' or 'graph_ids' parameter.\n",
    "\n",
    "The 'split_train', 'split_test' and 'split_validation' parameters can either be ratios, percentages or relative size compared to total. \n",
    "\n",
    "We opt to create a test, train _and_ validation set to use in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: CustomSpektralDataset(n_graphs=791)\n",
      "Test: CustomSpektralDataset(n_graphs=477)\n",
      "Validation: CustomSpektralDataset(n_graphs=336)\n"
     ]
    }
   ],
   "source": [
    "train, test, val = dataset.split_test_train_validation(\n",
    "    split_train=4, split_test=1, split_validation=1, by_graph_id=True, random_seed=42\n",
    ")\n",
    "print(\"Train:\", train)\n",
    "print(\"Test:\", test)\n",
    "print(\"Validation:\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è We can see that, because we are splitting by only 4 different graph_ids here (the 4 match_ids) the ratio's aren't perfectly 4 to 1 to 1. If you change the `graph_id=match_id` parameter in the `SoccerGraphConverter` to `graph_ids=dummy_graph_ids(dataset)` you'll see that it's easier to get close to the correct ratios, simply because we have a lot more graph_ids to split a cross. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 5  # Increase for actual training\n",
    "batch_size = 32\n",
    "channels = 128\n",
    "n_layers = 3  # Number of CrystalConv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Build GNN Model\n",
    "\n",
    "This GNN Model has the same architecture as described in [A Graph Neural Network Deep-dive into Successful Counterattacks {A. Sahasrabudhe & J. Bekkers}](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn/tree/main)\n",
    "\n",
    "This exact model can also simply be loaded as:\n",
    "\n",
    "`from unravel.classifiers import CrystalGraphClassifier` as shown in [Quick Start Guide](0_quick_start_guide.ipynb)\n",
    "\n",
    "Below we show the exact same code to make it easier to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import GlobalAvgPool, CrystalConv\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "class CrystalGraphClassifier(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int = 3,\n",
    "        channels: int = 128,\n",
    "        drop_out: float = 0.5,\n",
    "        n_out: int = 1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.channels = channels\n",
    "        self.drop_out = drop_out\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.conv1 = CrystalConv()\n",
    "        self.convs = [CrystalConv() for _ in range(1, self.n_layers)]\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(self.channels, activation=\"relu\")\n",
    "        self.dropout = Dropout(self.drop_out)\n",
    "        self.dense2 = Dense(self.channels, activation=\"relu\")\n",
    "        self.dense3 = Dense(self.n_out, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a, e])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Create DataLoaders\n",
    "\n",
    "Create a Spektral [`DisjointLoader`](https://graphneural.network/loaders/#disjointloader). This DisjointLoader will help us to load batches of Disjoint Graphs for training purposes.\n",
    "\n",
    "Note that these Spektral `Loaders` return a generator, so if we want to retrain the model, we need to reload these loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import DisjointLoader\n",
    "\n",
    "loader_tr = DisjointLoader(train, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(val, epochs=1, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training and Prediction\n",
    "\n",
    "Below we outline how to train the model, make predictions and add the predicted values back to the Kloppy dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Compile Model\n",
    "\n",
    "1. Initialize the `CrystalGraphClassifier` (or create your own Graph Classifier).\n",
    "2. Compile the model with a loss function, optimizer and your preferred metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = CrystalGraphClassifier()\n",
    "\n",
    "model.compile(\n",
    "    loss=BinaryCrossentropy(), optimizer=Adam(), metrics=[AUC(), BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Fit Model\n",
    "\n",
    "1. We have a a [`DisjointLoader`](https://graphneural.network/loaders/#disjointloader) for training and validation sets.\n",
    "2. Fit the model. \n",
    "3. We add `EarlyStopping` and a `validation_data` dataset to monitor performance, and set `use_multiprocessing=True` to improve training speed.\n",
    "\n",
    "‚ö†Ô∏è When trying to fit the model _again_ make sure to reload Data Loaders in [Section 6.4](#64-create-dataloaders), because they are generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    loader_tr.load(),\n",
    "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
    "    epochs=5,\n",
    "    use_multiprocessing=True,\n",
    "    validation_data=loader_va.load(),\n",
    "    callbacks=[EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Save & Load Model\n",
    "\n",
    "This step is solely included to show how to restore a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = \"models/my-first-graph-classifier\"\n",
    "model.save(model_path)\n",
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Evaluate Model\n",
    "\n",
    "1. Create another `DisjointLoader`, this time for the test set.\n",
    "2. Evaluate model performance on the test set. This evaluation function uses the `metrics` passed to `model.compile`\n",
    "\n",
    "üóíÔ∏è Our performance is really bad because we're using random labels, very few epochs and a small dataset.\n",
    "\n",
    "üìñ For more information on evaluation in sports analytics see: [Methodology and evaluation in sports analytics: challenges, approaches, and lessons learned {J. Davis et. al. (2024)}](https://link.springer.com/article/10.1007/s10994-024-06585-0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step - loss: 0.7250 - auc: 0.5309 - binary_accuracy: 0.5241\n"
     ]
    }
   ],
   "source": [
    "loader_te = DisjointLoader(test, epochs=1, shuffle=False, batch_size=batch_size)\n",
    "results = model.evaluate(loader_te.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Predict on New Data\n",
    "\n",
    "1. Load new, unseen data from the SkillCorner dataset.\n",
    "2. Convert this data, making sure we use the exact same settings as in step 1.\n",
    "3. If we set `prediction=True` we do not have to supply labels to the `SoccerGraphConverter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kloppy_dataset = skillcorner.load_open_data(\n",
    "    match_id=2068,  # A game we have not yet used in section 4\n",
    "    include_empty_frames=False,\n",
    "    limit=500,\n",
    ")\n",
    "\n",
    "preds_converter = SoccerGraphConverter(\n",
    "    dataset=kloppy_dataset,\n",
    "    prediction=True,\n",
    "    ball_carrier_treshold=25.0,\n",
    "    max_player_speed=12.0,\n",
    "    max_ball_speed=28.0,\n",
    "    boundary_correction=None,\n",
    "    self_loop_ball=True,\n",
    "    adjacency_matrix_connect_type=\"ball\",\n",
    "    adjacency_matrix_type=\"split_by_team\",\n",
    "    label_type=\"binary\",\n",
    "    infer_ball_ownership=True,\n",
    "    infer_goalkeepers=True,\n",
    "    defending_team_node_value=0.1,\n",
    "    non_potential_receiver_node_value=0.1,\n",
    "    random_seed=False,\n",
    "    pad=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a prediction on all the frames of this dataset using `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:01<00:00, 326.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Compute the graphs and add them to the GraphDataset\n",
    "pred_dataset = GraphDataset(graphs=preds_converter.to_spektral_graphs())\n",
    "\n",
    "loader_pred = DisjointLoader(\n",
    "    pred_dataset, batch_size=batch_size, epochs=1, shuffle=False\n",
    ")\n",
    "preds = model.predict(loader_pred.load(), use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Convert Klopy dataset to a dataframe and merge back the pedictions using the frame_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_id</th>\n",
       "      <th>period_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>2166</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:33.300000</td>\n",
       "      <td>0.259016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>2167</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:33.400000</td>\n",
       "      <td>0.251124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>2168</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:33.500000</td>\n",
       "      <td>0.258305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>2169</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:33.600000</td>\n",
       "      <td>0.256378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>2170</td>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:33.700000</td>\n",
       "      <td>0.305434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame_id  period_id              timestamp         y\n",
       "300      2166          1 0 days 00:00:33.300000  0.259016\n",
       "301      2167          1 0 days 00:00:33.400000  0.251124\n",
       "302      2168          1 0 days 00:00:33.500000  0.258305\n",
       "303      2169          1 0 days 00:00:33.600000  0.256378\n",
       "304      2170          1 0 days 00:00:33.700000  0.305434"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kloppy_df = kloppy_dataset.to_df()\n",
    "\n",
    "preds_df = pd.DataFrame(\n",
    "    {\"frame_id\": [x.id for x in pred_dataset], \"y\": preds.flatten()}\n",
    ")\n",
    "\n",
    "kloppy_df = pd.merge(kloppy_df, preds_df, on=\"frame_id\", how=\"left\")\n",
    "\n",
    "kloppy_df[300:305][[\"frame_id\", \"period_id\", \"timestamp\", \"y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóíÔ∏è Not all frames have a prediction because of missing (ball) data, so we look at the 300th-305th frame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
