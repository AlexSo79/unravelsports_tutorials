{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ€ It's all starting to Unravel!\n",
    "\n",
    "First run `pip install unravelsports` if you haven't already!\n",
    "\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unravelsports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "This notebook shows how to use this package to convert [Kloppy](https://github.com/PySport/kloppy) tracking data format into Graphs. These Graphs can subsequently be used to train a Graph Neural Network with the [Spektral](https://graphneural.network/) library as discussed in [A Graph Neural Network Deep-dive into Successful Counterattacks {A. Sahasrabudhe & J. Bekkers}](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn/tree/main).\n",
    "\n",
    "This example follows these steps:\n",
    "- [2. Imports](#2-imports)\n",
    "- [3. Open SkillCorner Data](#3-open-skillcorner-data)\n",
    "- [4. Graph Neural Network Converter](#4-graph-neural-network-converter)\n",
    "- [5. Load Kloppy Data, Convert & Store](#5-load-kloppy-data-convert-and-store)\n",
    "- [6. Creating a Custom Graph Dataset](#6-creating-a-custom-graph-dataset)\n",
    "- [7. Prepare for Training](#7-prepare-for-training)\n",
    "- [8. GNN Training](#8-training-gnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Imports\n",
    "\n",
    "We import `GraphConverter` to help us convert from Kloppy tracking data frames to graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unravel.soccer import GraphConverter\n",
    "\n",
    "from kloppy import skillcorner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### 3. Open SkillCorner Data\n",
    "\n",
    "The `GraphConverter` class supports the conversion of every tracking data provider supported by [PySports Kloppy](https://github.com/PySport/kloppy), namely:\n",
    "- Sportec\n",
    "- Tracab\n",
    "- SecondSpectrum\n",
    "- SkillCorner\n",
    "- StatsPerform\n",
    "- Metrica\n",
    "\n",
    "In this example we're going to use tracking data frames from 4 matches of [Open SkillCorner Data](https://github.com/SkillCorner/opendata). \n",
    "\n",
    "All we need to know for now is that this data is from the following matches:\n",
    "\n",
    "|    | date_time            |   id | home_team.short_name   | away_team.short_name   |\n",
    "|---:|:---------------------|-----:|:-----------------------|:-----------------------|\n",
    "|  0 | 2020-07-02T19:15:00Z | 4039 | Manchester City        | Liverpool              |\n",
    "|  1 | 2020-05-26T16:30:00Z | 3749 | Dortmund               | Bayern Munchen         |\n",
    "|  2 | 2020-03-08T19:45:00Z | 3518 | Juventus               | Inter                  |\n",
    "|  3 | 2020-03-01T20:00:00Z | 3442 | Real Madrid            | FC Barcelona           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### 4. Graph Neural Network Converter\n",
    "\n",
    "To get started with the `GraphConverter` we need to pass two required parameters:\n",
    "- `dataset` (of type `TrackingDataset` (Kloppy)) \n",
    "- `labels` (a dict with `frame_id` as keys and a value of `{True, False, 1 or 0}`). For example {frame_id: 1, frame_id: 0}. You will need to create your own labels! In this example we'll use `dummy_labels(dataset)` to generate a fake label for each frame.\n",
    "\n",
    "\n",
    "#### Graph Identifier(s):\n",
    "When training a GNN it's highly recommended to split data into test/train(/validation) on match level, or sequence/possession level such that all values from one match/sequence/possession all end up in the same test, train or validation set. This should be done to avoid leaking information between test, train and validation sets.\n",
    "\n",
    "To make this simple we have two options we can pass to `GraphConverter`, namely:\n",
    "- `graph_id`. This is a single identifier (str or int) for a whole match, for example the unique match id.\n",
    "- `graph_ids`. This is a dictionary with the same keys as `labels`, but the values are now the unique identifiers. This option can be used if we want to split by sequence or possession_id. For example: {frame_id: 'matchId-sequenceId', frame_id: 'match_Id-sequenceId2'} etc. You will need to create your own possession/sequence ids. Note, if `labels` and `graph_ids` don't have the exact same keys it will throw an error. In this example we'll use the `graph_id=match_id` as the unique identifier, but feel free to change that for `graph_ids=dummy_graph_ids(dataset)` to test out that behavior.\n",
    "\n",
    "Correctly splitting the final dataset in train, test and validiation sets is incorporated into `CustomGraphDataset` (see section 7 for more information).\n",
    "\n",
    "\n",
    "#### Graph Converter Settings:\n",
    "| Parameter | Type | Description | Default |\n",
    "|-----------|------|-------------|---------|\n",
    "| `ball_carrier_threshold` | float | The distance threshold to determine the ball carrier in meters. If no ball carrier within ball_carrier_threshold, we skip the frame. | 25.0 |\n",
    "| `max_player_speed` | float | The maximum speed of a player in meters per second. Used for normalizing node features. | 12.0 |\n",
    "| `max_ball_speed` | float | The maximum speed of the ball in meters per second. Used for normalizing node features. | 28.0 |\n",
    "| `boundary_correction` | float | A correction factor for boundary calculations, used to correct out of bounds as a percentage (Used as 1+boundary_correction, i.e., 0.05). Not setting this might lead to players outside the pitch markings to have values that fall slightly outside of our normalization range. When we set boundary_correction, any players outside the pitch will be moved to be on the closest line. | None |\n",
    "| `self_loop_ball` | bool | Flag to indicate if the ball node should have a self-loop, aka be connected with itself and not only player(s) | True |\n",
    "| `adjacency_matrix_connect_type` | str | The type of connection used in the adjacency matrix, typically related to the ball. Choose from 'ball', 'ball_carrier' or 'no_connection' | 'ball' |\n",
    "| `adjacency_matrix_type` | str | The type of adjacency matrix, indicating how connections are structured, such as split by team. Choose from 'delaunay', 'split_by_team', 'dense', 'dense_ap' or 'dense_dp' | 'split_by_team' |\n",
    "| `infer_ball_ownership` | bool | Infers 'attacking_team' if no 'ball_owning_team' exist (in Kloppy TrackingDataset) by finding the player closest to the ball using ball xyz, uses 'ball_carrier_threshold' as a cut-off. | True |\n",
    "| `infer_goalkeepers` | bool | Set True if no GK label is provided, set False for incomplete (broadcast tracking) data that might not have a GK in every frame. | True |\n",
    "| `defending_team_node_value` | float | Value for the node feature when player is on defending team. Should be between 0 and 1 including. | 0.1 |\n",
    "| `non_potential_receiver_node_value` | float | Value for the node feature when player is NOT a potential receiver of a pass (when on opposing team or in possession of the ball). Should be between 0 and 1 including. | 0.1 |\n",
    "| `label_type` | str | The type of prediction label used. Currently only supports 'binary' | 'binary' |\n",
    "| `random_seed` | int, bool | When a random_seed is given, it will randomly shuffle an individual Graph without changing the underlying structure. When set to True, it will shuffle every frame differently; False won't shuffle. Advised to set True when creating an actual dataset to support Permutation Invariance. | False |\n",
    "| `pad` | bool | True pads to a total amount of 22 players and ball (so 23x23 adjacency matrix). It dynamically changes the edge feature padding size based on the combination of AdjacencyMatrixConnectType and AdjacencyMatrixType, and self_loop_ball. No need to set padding because smaller and larger graphs can all be used in the same dataset. | False |\n",
    "| `verbose` | bool | The converter logs warnings / error messages when specific frames have no coordinates, or other missing information. False mutes all of these warnings. | False |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### 5. Load Kloppy Data, Convert and Store\n",
    "\n",
    "Here we loop over 4 SkillCorner matches and convert the first 500 frames.\n",
    "\n",
    "Important things to note:\n",
    "- We import `dummy_labels` to randomly generate binary labels.\n",
    "- We import `dummy_graph_ids` to renerage fake graph labels.\n",
    "- Our `GraphConverter` uses the Kloppy `DatasetTransformer` under the hood, which will take care of setting up playing orientation and coordinate system correctly.\n",
    "- Technically setting the coordinate system does not matter, because the `DatasetTransformer` transforms everything to `coordinates=\"secondspectrum\"` as said, but setting it already will speed up parsing a bit.\n",
    "- In this example we don't have any _actual_ labels for our tracking data frames, you are going to have to create your own. In this example we use `dummy_labels(dataset)` to randomly generate `True` or `False` labels for each frame. This also means training with these random labels will not create a good model.\n",
    "- We store the data as individual pickle files, one for each match. The data that gets stored in the pickle is a list of dicts. One dict per frame. Each dict has keys:\n",
    "    - 'a' (adjacency matrix) [np.array of shape (players+ball, players+ball)\n",
    "    - 'x' (node features) [np.array of shape (n_nodes, n_node_features)]. The currently implemented node features (in order) are:\n",
    "        - normalized x-coordinate\n",
    "        - normalized y-coordinate\n",
    "        - x component of the velocity unit vector\n",
    "        - y component of the velocity unit vector\n",
    "        - normalized speed\n",
    "        - normalized angle of velocity vector\n",
    "        - normalized distance to goal\n",
    "        - normalized angle to goal\n",
    "        - normalized distance to ball\n",
    "        - normalized angle to ball\n",
    "        - attacking (1) or defending team (`defending_team_node_value`) \n",
    "        - potential receiver (1) else `non_potential_receiver_node_value` \n",
    "    - 'e' (edge features) [np.array of shape (np.non_zero(a), n_edge_features)]. The currently implemented edge features (in order) are:\n",
    "        - normalized inter-player distance\n",
    "        - normalized inter-player speed difference\n",
    "        - inter-player angle cosine\n",
    "        - inter-player angle sine\n",
    "        - inter-player velocity vector cosine\n",
    "        - inter-player velocity vector sine \n",
    "        - optional: 1 if two players are connected else 0 according to delaunay adjacency matrix. Only if adjacency_matrix_type is NOT 'delauney'\n",
    "    - 'y' (label) [np.array] \n",
    "- We will end up with fewer than 2,000 eventhough we set `limit=500` frames because we set `include_empty_frames=False`\n",
    "- When using different providers always set `include_empty_frames=False` or `only_alive=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "from unravel.utils import dummy_labels, dummy_graph_ids\n",
    "\n",
    "match_ids = [4039, 3749, 3518, 3442]\n",
    "pickle_file_path = \"pickle_files/{match_id}.pickle\"\n",
    "\n",
    "for match_id in match_ids:\n",
    "    match_pickle_file_path = pickle_file_path.format(match_id=match_id)\n",
    "    # if the output file already exists, skip this whole step\n",
    "    if not exists(match_pickle_file_path):\n",
    "\n",
    "        # Load Kloppy dataset\n",
    "        dataset = skillcorner.load_open_data(\n",
    "            match_id=match_id,\n",
    "            coordinates=\"secondspectrum\",\n",
    "            include_empty_frames=False,\n",
    "            limit=500,  # limit to 500 frames in this example\n",
    "        )\n",
    "\n",
    "        # Initialize the GNN Converter, with dataset, labels and settings\n",
    "        converter = GraphConverter(\n",
    "            dataset=dataset,\n",
    "            # create fake labels\n",
    "            labels=dummy_labels(dataset),\n",
    "            graph_id=match_id,\n",
    "            # graph_ids=dummy_graph_ids(dataset),\n",
    "            # settings\n",
    "            ball_carrier_treshold=25.0,\n",
    "            max_player_speed=12.0,\n",
    "            max_ball_speed=28.0,\n",
    "            boundary_correction=None,\n",
    "            self_loop_ball=False,\n",
    "            adjacency_matrix_connect_type=\"ball\",\n",
    "            adjacency_matrix_type=\"split_by_team\",\n",
    "            label_type=\"binary\",\n",
    "            infer_ball_ownership=True,\n",
    "            infer_goalkeepers=True,\n",
    "            defending_team_node_value=0.1,\n",
    "            non_potential_receiver_node_value=0.1,\n",
    "            random_seed=False,\n",
    "            pad=True,\n",
    "            verbose=False,\n",
    "        )\n",
    "        # Compute the graphs and directly store them as a pickle file\n",
    "        converter.to_pickle(file_path=match_pickle_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### 6. Creating a Custom Graph Dataset\n",
    "\n",
    "- `CustomGraphDataset` (or `CounterDataset` as it's named in [U.S. Soccer Federation GNN Repository](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn/blob/main/counterattack.ipynb)) is a [`spektral.data.Dataset`](https://graphneural.network/creating-dataset/). \n",
    "This type of dataset is required to properly load and train a Spektral GNN.\n",
    "- The `CustomGraphDataset` has a custom method `add()` that allows us to update to add more Graphs. This is useful because we can load an individual match pickle file and add/update the graphs directly to `dataset`. (Note this `dataset` is different than the previoulsy loaded Kloppy dataset!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Deserialize the object from the file\n",
    "        graph_data = pickle.load(file)\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 477 graphs into CustomGraphDataset...\n",
      "Loading 477 graphs into CustomGraphDataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 380 graphs to CustomGraphDataset...\n",
      "Adding 336 graphs to CustomGraphDataset...\n",
      "Adding 411 graphs to CustomGraphDataset...\n",
      "Complete: CustomGraphDataset(n_graphs=1604)\n"
     ]
    }
   ],
   "source": [
    "from unravel.soccer import CustomGraphDataset\n",
    "\n",
    "dataset: CustomGraphDataset = None\n",
    "\n",
    "for match_id in match_ids:\n",
    "    graph_data = load_pickle(file_path=pickle_file_path.format(match_id=match_id))\n",
    "\n",
    "    if not dataset:\n",
    "        dataset = CustomGraphDataset(data=graph_data)\n",
    "    else:\n",
    "        dataset.add(graph_data, verbose=True)\n",
    "\n",
    "print(\"Complete:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### 7. Prepare for Training\n",
    "\n",
    "Now that we have all the data converted as Graphs inside our `CustomGraphDataset` object, we can prepare to train the GNN model.\n",
    "\n",
    "We first get all necessary information from our dataset that we need to train our model, namely:\n",
    "- N = Max amount of nodes in a single graph\n",
    "- F = Number of of Node Features\n",
    "- S = Number of of Edge Features\n",
    "- n_out = Dimesion of the target\n",
    "- n = Number of samples in dataset\n",
    "\n",
    "Please ignore the 'weird' naming convention, this simply copies the Spektral documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, F, S, n_out, n = dataset.dimensions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Split Dataset\n",
    "\n",
    "Our `dataset` object has two custom methods to help split the data into train, test and validation sets.\n",
    "Either use `dataset.split_test_train()` if we don't need a validation set, or `dataset.split_test_train_validation()` if we do also require a validation set.\n",
    "\n",
    "We can split our data amongst these subsets 'by_graph_id' if we have provided Graph Ids in our `GraphConverter` using the 'graph_id' or 'graph_ids' parameter.\n",
    "The 'split_train', 'split_test' and 'split_validation' parameters can either be ratios, percentages or relative size compared to total. \n",
    "\n",
    "Note: We can see that, because we are splitting by only 4 different graph_ids here (the 4 match_ids) the ratio's aren't perfectly 4 to 1 to 1. If you change the `graph_id=match_id` parameter in the `GraphConverter` to `graph_ids=dummy_graph_ids(dataset)` you'll see that it's easier to get close to the correct ratios, simply because we have a lot more graph_ids to split a cross. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: CustomGraphDataset(n_graphs=791)\n",
      "Test: CustomGraphDataset(n_graphs=477)\n",
      "Validation: CustomGraphDataset(n_graphs=336)\n"
     ]
    }
   ],
   "source": [
    "train, test, val = dataset.split_test_train_validation(\n",
    "    split_train=4, split_test=1, split_validation=1, by_graph_id=True, random_seed=42\n",
    ")\n",
    "print(\"Train:\", train)\n",
    "print(\"Test:\", test)\n",
    "print(\"Validation:\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 5  # Increase for actual training\n",
    "batch_size = 32\n",
    "channels = 128\n",
    "n_layers = 3  # Number of CrystalConv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Create DataLoaders\n",
    "\n",
    "Create a Spektral [`DisjointLoader`](https://graphneural.network/loaders/#disjointloader). This DisjointLoader will help us to load batches of Graphs for training purposes.\n",
    "\n",
    "We'll skip creating the validation loader for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import DisjointLoader\n",
    "\n",
    "loader_tr = DisjointLoader(train, batch_size=batch_size, epochs=epochs)\n",
    "loader_te = DisjointLoader(test, batch_size=batch_size, epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Build GNN Model\n",
    "\n",
    "This GNN Model has the same architecture as described in [A Graph Neural Network Deep-dive into Successful Counterattacks {A. Sahasrabudhe & J. Bekkers}](https://github.com/USSoccerFederation/ussf_ssac_23_soccer_gnn/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import GlobalAvgPool, CrystalConv\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class GNN(Model):\n",
    "    \"\"\"\n",
    "    Building the Graph Neural Network configuration with Model as the parent class\n",
    "    from spektral library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, channels, n_out):\n",
    "        \"\"\"\n",
    "        Constructor code for setting up the layers needed for training the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = CrystalConv()\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(CrystalConv())\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation=\"relu\")\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(channels, activation=\"relu\")\n",
    "        self.dense3 = Dense(n_out, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Build the neural network.\n",
    "        \"\"\"\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a, e])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = GNN(n_layers=n_layers, channels=channels, n_out=n_out)\n",
    "# Setup the optimizer\n",
    "optimizer = Adam(learning_rate)\n",
    "# Set up the logloss function\n",
    "loss_fn = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "### 8. Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.872689723968506\n",
      "Loss: 1.6423248052597046\n",
      "Loss: 0.9415183067321777\n",
      "Loss: 0.7331467270851135\n",
      "Loss: 0.7081347107887268\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Ensure eager execution is enabled\n",
    "@tf.function(input_signature=loader_tr.tf_signature())\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Print loss at each step of training.\n",
    "step = loss = 0\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss += train_step(*batch)\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
    "        loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
